{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence similarity using tensorflow hub models\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "tf.disable_eager_execution()\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import operator\n",
    "import pickle as pk\n",
    "import json\n",
    "import tensorflow_hub as hub\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "class Unbox_sm(object):\n",
    "    \n",
    "    \n",
    "    # loading universal sentence encoder \n",
    "    # to find similar sentences/paragraphs\n",
    "    \n",
    "    \n",
    "    model_dict = {\n",
    "                  \"elmo\": \"https://tfhub.dev/google/elmo/\" , \n",
    "                  \"use_m\": \"https://tfhub.dev/google/universal-sentence-encoder/\",\n",
    "                  \"use_l\": \"https://tfhub.dev/google/universal-sentence-encoder-large/\"\n",
    "                  }\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(model_type, version):\n",
    "        \n",
    "    #  model_dict[model_type] + str(version)\n",
    "    # some tfhub models support '.load' and some '.Module' \n",
    "\n",
    "        if model_type == 'elmo':\n",
    "            \n",
    "            # elmo model \n",
    "            # version : 1, 2, 3\n",
    "            module_url = \"https://tfhub.dev/google/elmo/\" + str(version)\n",
    "            return hub.Module(module_url)\n",
    "            \n",
    "        elif model_type == 'use_m':\n",
    "            sd_ver = ['1','2', 1, 2]\n",
    "            if version in sd_ver:\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder/\" + str(version)\n",
    "                return hub.Module(module_url)\n",
    "            else:\n",
    "\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder/\" + str(version)\n",
    "                return hub.load(module_url)\n",
    "            \n",
    "            # universal sentence encoder medium\n",
    "            # version : 1 , 2 , 3 , 4\n",
    "            \n",
    "        elif model_type == 'use_l':\n",
    "            \n",
    "            sd_ver = ['1','2', '3', 1, 2,3 ]\n",
    "            \n",
    "            if version in sd_ver:\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/\" + str(version)\n",
    "                return hub.Module(module_url)\n",
    "            else:\n",
    "\n",
    "                module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/\" + str(version)\n",
    "                return hub.load(module_url)\n",
    "            \n",
    "            \n",
    "            #universal sentence encoder large\n",
    "            # version : 1 , 2 , 3 , 4, 5\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(model_type, version, dataset):\n",
    "        \n",
    "        model = Unbox_sm.load_model(model_type, version)\n",
    "        \n",
    "        similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "        similarity_message_encodings = model(similarity_input_placeholder)\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "\n",
    "            message_embeddings = session.run(similarity_message_encodings,\n",
    "                                            feed_dict={similarity_input_placeholder: dataset})\n",
    "            \n",
    "        \n",
    "        return message_embeddings\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    # cosine distance\n",
    "    def similarity_score(vector_one, vector_2):\n",
    "        result = 1 - spatial.distance.cosine(vector_one, vector_2)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    #preprocessing\n",
    "    @staticmethod\n",
    "    def preprocessing_stopwords(sentence):\n",
    "        cleaned = [i.lower() for i in word_tokenize(sentence) if i not in stop]\n",
    "        return \" \".join(cleaned)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def unbox_sm(query, config_file = False ):\n",
    "        \n",
    "        default_config = {\n",
    "                          'model_type'     : 'use_l', \n",
    "                          'version'        : '5',\n",
    "                          'preprocessing'  :  False\n",
    "                         }\n",
    "        \n",
    "        \n",
    "        if config_file:\n",
    "            default_config.update(config_file)\n",
    "            \n",
    "        \n",
    "        if default_config['preprocessing']:\n",
    "            \n",
    "            query_vector_a =  preprocessing_stopwords(query['query_a'].lower())\n",
    "            query_vector_b =  preprocessing_stopwords(query_['query_b'].lower())\n",
    "        else:\n",
    "            query_vector_a =  query['query_a'].lower()\n",
    "            query_vector_b =  query_['query_b'].lower()\n",
    "            \n",
    "        \n",
    "        # get the embeddings\n",
    "        combine_query = [query_vector_a, query_vector_b ]\n",
    "        emb_s = get_embedding(default_config['model_type'], default_config['version'], dataset)\n",
    "        \n",
    "        # calculate the distance\n",
    "        sm_value = similarity_score(emb_s[0], emb_s[1])\n",
    "        \n",
    "        query['similarity_value'] = sm_value\n",
    "        return query\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_vectors(query, config_file = False ):\n",
    "        \n",
    "        default_config = {\n",
    "                          'model_type' : 'use_l', \n",
    "                          'version'    : '5' ,\n",
    "                          'preprocessing'  :  False\n",
    "                         }\n",
    "        \n",
    "        if config_file:\n",
    "            default_config.update(config_file)\n",
    "        \n",
    "        if default_config['preprocessing']:\n",
    "            query = [preprocessing_stopwords(sentence.lower()) for sentence in query]\n",
    "            \n",
    "        \n",
    "        # get the embeddings\n",
    "        emb_s = get_embedding(default_config['model_type'], default_config['version'], query)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = {\n",
    "                  'embeddings'      : np.array(emb_s), \n",
    "                  'total_queries'   : len(query), \n",
    "                  'total_embeddings': len(emb_s), \n",
    "                  'shape'           : np.array(emb_s).shape\n",
    "                 }\n",
    "        \n",
    "    \n",
    "    # pandas dataframe as output\n",
    "    @staticmethod\n",
    "    def get_bulk_vectors(query, config_file = False ):\n",
    "        \n",
    "        default_config = {\n",
    "                          'model_type' : 'use_l', \n",
    "                          'version'    : '5' ,\n",
    "                          'preprocessing'  :  False\n",
    "                         }\n",
    "        \n",
    "        if config_file:\n",
    "            default_config.update(config_file)\n",
    "        \n",
    "        \n",
    "        if default_config['preprocessing']:\n",
    "            query = [preprocessing_stopwords(sentence.lower()) for sentence in query]\n",
    "        \n",
    "        # get the embeddings\n",
    "        emb_s = get_embedding(default_config['model_type'], default_config['version'], query)\n",
    "        \n",
    "        output_ = {\n",
    "                  'embeddings'      : emb_s, \n",
    "                  'total_queries'   : len(query), \n",
    "                  'total_embeddings': len(emb_s), \n",
    "                  'shape'           : np.array(emb_s).shape,\n",
    "                  'queries'         : query\n",
    "                 }\n",
    "        \n",
    "        return pd.DataFrame(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = {'query_a': 'Hello how are you', 'query_b': 'Hello I am fine' }\n",
    "\n",
    "result = Unbox_sm.unbox_sm(querys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, version):\n",
    "        \n",
    "#  model_dict[model_type] + str(version)\n",
    "# some tfhub models support '.load' and some '.Module' \n",
    "\n",
    "    if model_type == 'elmo':\n",
    "\n",
    "        # elmo model \n",
    "        # version : 1, 2, 3\n",
    "        module_url = \"https://tfhub.dev/google/elmo/\" + str(version)\n",
    "        return hub.Module(module_url)\n",
    "\n",
    "    elif model_type == 'use_m':\n",
    "        sd_ver = ['1','2', 1, 2]\n",
    "        if version in sd_ver:\n",
    "            module_url = \"https://tfhub.dev/google/universal-sentence-encoder/\" + str(version)\n",
    "            return hub.Module(module_url)\n",
    "        else:\n",
    "\n",
    "            module_url = \"https://tfhub.dev/google/universal-sentence-encoder/\" + str(version)\n",
    "            return hub.load(module_url)\n",
    "\n",
    "        # universal sentence encoder medium\n",
    "        # version : 1 , 2 , 3 , 4\n",
    "\n",
    "    elif model_type == 'use_l':\n",
    "\n",
    "        sd_ver = ['1','2', '3', 1, 2,3 ]\n",
    "\n",
    "        if version in sd_ver:\n",
    "            module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/\" + str(version)\n",
    "            return hub.Module(module_url)\n",
    "        else:\n",
    "\n",
    "            module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/\" + str(version)\n",
    "            return hub.load(module_url)\n",
    "\n",
    "\n",
    "        #universal sentence encoder large\n",
    "        # version : 1 , 2 , 3 , 4, 5\n",
    "def get_embedding(model_type, version, dataset):\n",
    "        \n",
    "    model = Unbox_sm.load_model(model_type, version)\n",
    "\n",
    "    similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "    similarity_message_encodings = model(similarity_input_placeholder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "\n",
    "        message_embeddings = session.run(similarity_message_encodings,\n",
    "                                        feed_dict={similarity_input_placeholder: dataset})\n",
    "\n",
    "\n",
    "    return message_embeddings\n",
    "\n",
    "def similarity_score(vector_one, vector_2):\n",
    "    result = 1 - spatial.distance.cosine(vector_one, vector_2)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "def preprocessing_stopwords(sentence):\n",
    "    cleaned = [i.lower() for i in word_tokenize(sentence) if i not in stop]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unbox_sm(query, config_file = False ):\n",
    "\n",
    "    default_config = {\n",
    "                      'model_type'     : 'use_l', \n",
    "                      'version'        : '5',\n",
    "                      'preprocessing'  :  False\n",
    "                     }\n",
    "\n",
    "\n",
    "    if config_file:\n",
    "        default_config.update(config_file)\n",
    "\n",
    "\n",
    "    if default_config['preprocessing']:\n",
    "\n",
    "        query_vector_a =  preprocessing_stopwords(query['query_a'].lower())\n",
    "        query_vector_b =  preprocessing_stopwords(query['query_b'].lower())\n",
    "        \n",
    "        print(query_vector_a,query_vector_b)\n",
    "    else:\n",
    "        query_vector_a =  query['query_a'].lower()\n",
    "        query_vector_b =  query['query_b'].lower()\n",
    "\n",
    "    print(query_vector_a,query_vector_b)\n",
    "    # get the embeddings\n",
    "    combine_query = [query_vector_a, query_vector_b ]\n",
    "    emb_s = get_embedding(default_config['model_type'], default_config['version'], dataset)\n",
    "\n",
    "    # calculate the distance\n",
    "    sm_value = similarity_score(emb_s[0], emb_s[1])\n",
    "\n",
    "    query['similarity_value'] = sm_value\n",
    "    return query\n",
    "\n",
    "def get_vectors(query, config_file = False ):\n",
    "\n",
    "    default_config = {\n",
    "                      'model_type' : 'use_l', \n",
    "                      'version'    : '5' ,\n",
    "                      'preprocessing'  :  False\n",
    "                     }\n",
    "\n",
    "    if config_file:\n",
    "        default_config.update(config_file)\n",
    "\n",
    "    if default_config['preprocessing']:\n",
    "        query = [preprocessing_stopwords(sentence.lower()) for sentence in query]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # get the embeddings\n",
    "    emb_s = get_embedding(default_config['model_type'], default_config['version'], query)\n",
    "\n",
    "\n",
    "\n",
    "    output = {\n",
    "              'embeddings'      : np.array(emb_s), \n",
    "              'total_queries'   : len(query), \n",
    "              'total_embeddings': len(emb_s), \n",
    "              'shape'           : np.array(emb_s).shape\n",
    "             }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# pandas dataframe as output\n",
    "\n",
    "def get_bulk_vectors(query, config_file = False ):\n",
    "\n",
    "    default_config = {\n",
    "                      'model_type' : 'use_l', \n",
    "                      'version'    : '5' ,\n",
    "                      'preprocessing'  :  False\n",
    "                     }\n",
    "\n",
    "    if config_file:\n",
    "        default_config.update(config_file)\n",
    "\n",
    "\n",
    "    if default_config['preprocessing']:\n",
    "        query = [preprocessing_stopwords(sentence.lower()) for sentence in query]\n",
    "\n",
    "    # get the embeddings\n",
    "    emb_s = get_embedding(default_config['model_type'], default_config['version'], query)\n",
    "\n",
    "    output_ = {\n",
    "              'embeddings'      : emb_s, \n",
    "              'total_queries'   : len(query), \n",
    "              'total_embeddings': len(emb_s), \n",
    "              'shape'           : np.array(emb_s).shape,\n",
    "              'queries'         : query\n",
    "             }\n",
    "\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working\n",
    "load_f = load_model('use_l',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working\n",
    "dataset = ['Hello how are you', 'Hello I am fine', 'Hello how are you', 'Hello I am fine']\n",
    "res = get_embedding('use_l',5, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'query_a': 'Hello how are you', 'query_b': 'Hello I am fine' }\n",
    "\n",
    "der = unbox_sm(query, config_file = {\n",
    "                      'model_type'     : 'use_l', \n",
    "                      'version'        : '5',\n",
    "                      'preprocessing'  :  False\n",
    "                     } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk checking\n",
    "\n",
    "datas = get_vectors(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_p = get_bulk_vectors(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = {'query_a': 'Hello how are you', 'query_b': 'Hello I am fine' }\n",
    "embed = transformer_model.unbox_transformer_sm(query, config_file = {'preprocessing': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['Hello how are you', 'Hello I am fine', 'Hello how are you', 'Hello I am fine']\n",
    "\n",
    "bulk = transformer_model.get_transformer_vectors(query = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_similarity.unbox_transformers_sm import Unbox_transformers\n",
    "transformer_model = Unbox_transformers()\n",
    "embed = transformer_model.unbox_transformer_sm(query = {\n",
    "                                                        'query_a': 'Hello how are you', \n",
    "                                                        'query_b': 'Hello I am fine' \n",
    "                                                        }, \n",
    "                                               config_file = {'preprocessing': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.similarity_score(bulk['embeddings'][1],bulk['embeddings'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ANANT/apal/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's going to take some time, depend on your internet speed ... try base model if taking long time\n",
      "model loaded\n",
      "{'query_a': 'Hello how are you', 'query_b': 'Hello I am fine', 'similarity_value': 0.7853398323059082}\n"
     ]
    }
   ],
   "source": [
    "from semantic_similarity import Unbox_transformers\n",
    "transformer_model = Unbox_transformers()\n",
    "\n",
    "embed = transformer_model.unbox_transformer_sm(query = {\n",
    "                                                        'query_a': 'Hello how are you', \n",
    "                                                        'query_b': 'Hello I am fine' \n",
    "                                                        }, \n",
    "                                               config_file = {'preprocessing': True})\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_similarity import Unbox_tensorflow_hub\n",
    "transformer_model = Unbox_tensorflow_hub()\n",
    "\n",
    "embed = transformer_model.unbox_sm(query = { 'query_a': 'Hello how are you', \n",
    "                                             'query_b': 'Hello I am fine' \n",
    "                                            }, \n",
    "                                  config_file = {'preprocessing': True})\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
